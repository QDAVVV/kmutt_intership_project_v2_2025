{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/hugo-dev/esirem-dev/4a/kmutt/kmutt_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/data_25ms_0%_6mfcc.json'\n",
    "SAVED_MODEL_PATH = './models/cnn2d/model.pt'\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 5\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(data_path: str, test_size: float = 0.05, val_size: float = 0.1):\n",
    "    # read data from .json file\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = json.load(f)\n",
    "    X, y, files = np.array(data['mfcc']), np.array(data['labels']), np.array(data['files'])\n",
    "\n",
    "    # create train/validation/test splits\n",
    "    X_train, X_test, y_train, y_test, files_train, files_test = train_test_split(\n",
    "        X, y, files, test_size=test_size, random_state=23)\n",
    "    X_train, X_val, y_train, y_val, files_train, files_val = train_test_split(\n",
    "        X_train, y_train, files_train, test_size=val_size, random_state=23)\n",
    "\n",
    "    # convert inputs from 2D to 3D arrays (number of segments, 13) -> (number of segments, 13, 1)\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_val = X_val[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, files_train, files_val, files_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    r\"\"\"Plots accuracy/loss for training/validation set as a function of the epochs.\n",
    "\n",
    "    Args:\n",
    "        history:\n",
    "            Training history of the model.\n",
    "\n",
    "    Returns:\n",
    "        : matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2)\n",
    "\n",
    "    # create accuracy subplot\n",
    "    axs[0].plot(history['acc'] if isinstance(history, dict) else history.history['acc'], label='acc')\n",
    "    axs[0].plot(history['val_acc'] if isinstance(history, dict) else history.history['val_acc'], label='val_acc')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[0].set_title('Accuracy evaluation')\n",
    "\n",
    "    # create loss subplot\n",
    "    axs[1].plot(history['loss'] if isinstance(history, dict) else history.history['loss'], label='loss')\n",
    "    axs[1].plot(history['val_loss'] if isinstance(history, dict) else history.history['val_loss'], label='val_loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].set_title('Loss evaluation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(\n",
    "        model: torch.nn.Module,\n",
    "        loss_fn,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    model.eval()  # set the module in evaluation mode\n",
    "    running_loss_per_epoch = 0.0\n",
    "    running_number_of_samples_per_epoch = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            inputs, labels = data  # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs).to(device)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            running_loss_per_epoch += loss.item() * len(labels)\n",
    "            running_number_of_samples_per_epoch += len(labels)\n",
    "    return running_loss_per_epoch / running_number_of_samples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    model.eval()  # set the module in evaluation mode\n",
    "    running_correct_samples_per_epoch = 0\n",
    "    running_number_of_samples_per_epoch = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            inputs, labels = data  # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            y_pred = torch.argmax(model(inputs), dim=1)\n",
    "            number_of_correct_predictions = sum(y_pred == labels).item()\n",
    "            running_correct_samples_per_epoch += number_of_correct_predictions\n",
    "            running_number_of_samples_per_epoch += len(labels)\n",
    "    return running_correct_samples_per_epoch / running_number_of_samples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "        model: torch.nn.Module,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    model.eval()  # set the module in evaluation mode\n",
    "    test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader):\n",
    "            inputs, y = data  # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, y = inputs.to(device), y.to(device)\n",
    "            y_pred = torch.argmax(model(inputs), dim=1)\n",
    "            test_preds.extend(list(y_pred.detach().numpy()))\n",
    "    return np.array(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pt_dataloader(X: np.ndarray, y: np.ndarray, batch_size: int = 32, shuffle: bool = False):\n",
    "    # torch wants the input in NCHW (instead of NHWC format)\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2), torch.tensor(y, dtype=torch.int64))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(1, 1), padding='valid')\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.bn1 = torch.nn.BatchNorm2d(64)\n",
    "        self.maxpool1 = torch.nn.MaxPool2d((1, 1), stride=(1, 1))\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(64, 32, kernel_size=(1, 1), padding='valid')\n",
    "        self.bn2 = torch.nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = torch.nn.MaxPool2d((1, 1), stride=(1, 1))\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(32 * 6 * 1, 64)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        # conv layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        # flatten output\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # dense layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # last layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, monitor: str = 'val_loss', min_delta: float = 0.0, patience: int = 1):\n",
    "        self.monitor = monitor\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_val_loss = float('inf')\n",
    "        self.max_val_acc = 0\n",
    "\n",
    "    def early_stop(self, val):\n",
    "        if self.monitor.endswith('loss'):\n",
    "            if val < self.min_val_loss:\n",
    "                self.min_val_loss = val\n",
    "                self.counter = 0\n",
    "            elif val > self.min_val_loss + self.min_delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False\n",
    "        elif self.monitor.endswith('acc'):\n",
    "            if val > self.max_val_acc:\n",
    "                self.max_val_acc = val\n",
    "                self.counter = 0\n",
    "            elif val < self.max_val_acc - self.min_delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    return True\n",
    "            return False\n",
    "        else:\n",
    "            raise ValueError(f'The only recognized quantities to be monitored are: loss, acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pt_model(num_classes: int):\n",
    "    model = CNN(num_classes)\n",
    "    # print(torchinfo.summary(model, input_size=(BATCH_SIZE, 1, 44, 13)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pt_model(\n",
    "        model: torch.nn.Module,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val=None,\n",
    "        y_val=None,\n",
    "        epochs: int = 40,\n",
    "        patience: int = 5,\n",
    "        batch_size: int = 32,\n",
    "        learning_rate: float = 0.001\n",
    "):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataloader = prepare_pt_dataloader(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = prepare_pt_dataloader(X_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    early_stopper = EarlyStopper(monitor='val_acc', min_delta=0.001, patience=patience)\n",
    "\n",
    "    #  creating history of logs\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # set the module in training mode  (you should set this here inside the `range(epochs)` loop)\n",
    "        print(f'Epoch: {epoch+1}: Is model in train mode?', model.training)\n",
    "        running_loss_per_epoch = 0.0\n",
    "        running_number_of_correct_predictions_per_epoch = 0\n",
    "        running_number_of_samples_per_epoch = 0\n",
    "        for batch_idx, data in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()  # reset the gradients\n",
    "\n",
    "            inputs, labels = data  # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs).to(device)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            loss.backward()  # compute gradients during backpropagation\n",
    "            optimizer.step()  # update the params\n",
    "\n",
    "            # epoch statistics\n",
    "            y_pred = torch.argmax(outputs, dim=1).to(device)\n",
    "            running_loss_per_epoch += loss.item() * len(labels)\n",
    "            running_number_of_correct_predictions_per_epoch += sum(y_pred == labels).item()\n",
    "            running_number_of_samples_per_epoch += len(labels)\n",
    "\n",
    "        train_loss = running_loss_per_epoch / running_number_of_samples_per_epoch\n",
    "        train_acc = running_number_of_correct_predictions_per_epoch / running_number_of_samples_per_epoch\n",
    "        val_loss = get_loss(model, loss_fn, val_dataloader, device=device)\n",
    "        val_acc = get_accuracy(model, val_dataloader, device=device)\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] \"\n",
    "              f\"Epoch {epoch+1}/{epochs}\\n\"\n",
    "              f\"{len(train_dataloader)}/{len(train_dataloader)} [==============================] \"\n",
    "              f\"loss: {train_loss:.4f} - acc: {train_acc:.4f} - \"\n",
    "              f\"val_loss: {val_loss:.4f} - val_acc: {val_acc:.4f} - lr: {optimizer.state_dict()['param_groups'][0]['lr']}\")\n",
    "\n",
    "        if early_stopper.early_stop(val_acc):\n",
    "            print(f'Early stopping criterion triggered: We did not have an increase in val_acc after {patience} epochs.')\n",
    "            break\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pt_model(\n",
    "        model: torch.nn.Module,\n",
    "        loss_fn,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        files_test,\n",
    "        mapping=None,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    test_dataloader = prepare_pt_dataloader(X_test, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "    # get accumulated statistics\n",
    "    test_loss = get_loss(model, loss_fn, test_dataloader, device=device)\n",
    "    test_acc = get_accuracy(model, test_dataloader, device=device)\n",
    "    print(f'\\ntest_loss: {test_loss:.4f}, test_acc: {test_acc:.4f}\\n')\n",
    "\n",
    "    # see specific examples\n",
    "    y_test_pred = predict(model, test_dataloader)\n",
    "    for file, y, yhat in zip(files_test, y_test, y_test_pred):\n",
    "        if y == yhat:\n",
    "            print(f'{file}: true: {y if mapping is None else mapping[y]}, '\n",
    "                  f'pred: {yhat if mapping is None else mapping[yhat]}')\n",
    "        else:\n",
    "            print(f'    WRONG!!! {file}: true: {y if mapping is None else mapping[y]}, '\n",
    "                  f'pred: {yhat if mapping is None else mapping[yhat]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (239459, 1, 6, 1), y_train shape: (239459,)\n",
      "Epoch: 1: Is model in train mode? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 325/7484 [00:01<00:43, 163.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m pt_model \u001b[38;5;241m=\u001b[39m build_pt_model(NUM_CLASSES)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m pt_model, pt_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pt_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# plot history\u001b[39;00m\n\u001b[1;32m     20\u001b[0m plot_history(pt_history)\n",
      "Cell \u001b[0;32mIn[41], line 41\u001b[0m, in \u001b[0;36mtrain_pt_model\u001b[0;34m(model, X_train, y_train, X_val, y_val, epochs, patience, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     38\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data  \u001b[38;5;66;03m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[1;32m     39\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m     44\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# compute gradients during backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/esirem-dev/4a/kmutt/kmutt_v2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/esirem-dev/4a/kmutt/kmutt_v2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 22\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# conv layer 1\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n",
      "File \u001b[0;32m~/esirem-dev/4a/kmutt/kmutt_v2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/esirem-dev/4a/kmutt/kmutt_v2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/esirem-dev/4a/kmutt/kmutt_v2/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/esirem-dev/4a/kmutt/kmutt_v2/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, 'rb') as f:\n",
    "    data = json.load(f)\n",
    "mapping = data['mapping']\n",
    "NUM_CLASSES = len(mapping)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, files_train, files_val, files_test = get_data_splits(DATA_PATH)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "# build the model\n",
    "pt_model = build_pt_model(NUM_CLASSES)\n",
    "\n",
    "# train the model\n",
    "pt_model, pt_history = train_pt_model(\n",
    "    pt_model, X_train, y_train, X_val=X_val, y_val=y_val,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# plot history\n",
    "plot_history(pt_history)\n",
    "\n",
    "# evaluate the model on test data\n",
    "eval_pt_model(pt_model, torch.nn.CrossEntropyLoss(), X_test, y_test, files_test, mapping=mapping)\n",
    "\n",
    "# save the model\n",
    "scripted_model = torch.jit.script(pt_model)\n",
    "scripted_model.save(SAVED_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
